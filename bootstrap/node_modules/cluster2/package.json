{
  "author": {
    "name": "ql.io"
  },
  "contributors": [
    {
      "name": "Subbu Allamaraju",
      "email": "subbu@ebaysf.com"
    }
  ],
  "name": "cluster2",
  "version": "0.3.1",
  "repository": {
    "type": "git",
    "url": "https://github.com/ql-io/cluster2"
  },
  "engines": {
    "node": ">= 0.6.0"
  },
  "main": "lib/index.js",
  "dependencies": {
    "underscore": "",
    "express": "2.5.9",
    "ejs": "",
    "npm": ""
  },
  "devDependencies": {
    "express": "2.5.9",
    "nodeunit": "",
    "request": ""
  },
  "scripts": {
    "test": "nodeunit test"
  },
  "optionalDependencies": {},
  "readme": "## What is cluster2\n\n![Travis status](https://secure.travis-ci.org/ql-io/cluster2.png)\n\ncluster2 is a node.js (>= 0.6.x) compatible multi-process management module. This module grew out of\nour needs in operationalizing node.js for [ql.io](https://github.com/ql-io/ql.io) at eBay. Built on\nnode's `cluster`, cluster2 adds several safeguards and utility functions to help support real-world\nproduction scenarios:\n\n* Scriptable start, shutdown and stop flows\n* Worker monitoring for process deaths\n* Worker recycling\n* Graceful shutdown\n* Idle timeouts\n* Validation hooks (for other tools to monitor cluster2 apps)\n* Events for logging cluster activities\n* Exit with error code when the port is busy to fail start scripts\n* Disable monitor\n* and more coming soon\n\n## Usage\n\n### Getting cluster2\n\n    npm install cluster2\n\n### Start a TCP Server\n\n    var Cluster = require('cluster2'),\n        net = require('net');\n    var server = net.createServer(function (c) {\n        c.on('end', function () {\n            console.log('server disconnected');\n        });\n        c.write('hello\\r\\n');\n        c.pipe(c);\n    });\n\n    var c = new Cluster({\n        port: 3000,\n        cluster: true\n    });\n\n### Start a HTTP Server\n\n    var Cluster = require('cluster2'),\n        http = require('http');\n    var server = http.createServer(function (req, res) {\n        res.writeHead(200);\n        res.end('hello');\n    });\n    var c = new Cluster({\n        port: 3000\n    });\n    c.listen(function(cb) {\n        cb(server);\n    });\n\n### Start an Express Server\n\n    var Cluster = require('cluster2'),\n        express = require('express');\n    var app = express.createServer();\n    app.get('/', function(req, res) {\n        res.send('hello');\n    });\n\n    var c = new Cluster({\n        port: 3000,\n    });\n    c.listen(function(cb) {\n        cb(app);\n    });\n\n### Stop a Server\n\n    var Cluster = require('cluster2');\n    var c = new Cluster();\n    c.stop();\n\n### Gracefully Shutdown a Server\n\n    var Cluster = require('cluster2');\n    var c = new Cluster();\n    c.shutdown();\n\n\n## Options\n\nCluster2 takes the following options.\n\n* `cluster`: When `true` starts a number of workers. Use `false` to start the server as a single\n   process. Defaults to `true`.\n* `pids`: A directory to write PID files for master and workers.\n* `port`: Port number for the app, defaults to `3000`.\n* `monPort`: Port number for the monitor URL, defaults to `3001`. Go to `http://<localhost>:3001` to\n   view application logs (whatever is written to a `/logs` dir), and npm dependencies.\n* `ecv`: ECV stands for \"extended content verification\". This is an object with the following\n   additional properties:\n     * `path`: A path to serve a heart beat. See below.\n     * `monitor`: A URI to check before emitting a valid heart beat signal\n     * `control`: When true, allows clients to enable or disable the signal. See below.\n     validator to validate the runtime health of the app. If found unhealthy, emits a disable\n* `noWorkers`: Defaults to `os.cpus().length`.\n* `timeout`: Idle socket timeout. Automatically ends incoming sockets if found idle for this\n   duration. Defaults to `30` seconds.\n* `connThreshold`: When the number of connections processed exceeds this numbers, recycle the worker\n   process. This can help recover from slow leaks in your code or dependent modules.\n\n## Graceful Shutdown\n\nThe purpose of `shutdown()` is to let the server reject taking new connections, handle all pending\nrequests and end the connecton so that no request dropped. In order to handling `shutdown()`, the\nserver must handle `close` events as follows.\n\n    var serving = true;\n    var server = http.createServer(function (req, res) {\n        if(!serving) {\n            // Be nice and send a connection: close as otherwise the client may pump more requests\n            // on the same connection\n            res.writeHead(200, {\n                'connection': 'close'\n            });\n        }\n        res.writeHead(200);\n        res.end('hello');\n    });\n    server.on('close', function() {\n        serving = false;\n    })\n    var c = new Cluster({\n        port: 3000,\n        cluster: true\n    });\n\nCompletion of `shutdown()` does not necessarily mean that all worker processes are dead immediately. \nThe workers may take a while to complete processing of current requests and exit. The `shutdown()` \nflow only guarantees that the server takes no new connections.\n\n## Cluster2 Events\n\nCluster2 is an `EventEmitter` and emits the following events.\n\n* `died`: Emitted when a worker dies. This event is also emitted during normal `shutdown()` or\n  `stop()`.\n* `forked`: Emitted when a new worker is forked.\n* `<signal>`: Emitted when a worker receives a signal (such as `SIGKILL`, `SIGTERM` or `SIGINT`).\n\nHere is an example that logs these events to the disk.\n\n    var Cluster = require('cluster2'),\n        http = require('http');\n\n    var server = http.createServer(function (req, res) {\n        res.writeHead(200);\n        res.end('hello');\n    });\n    var c = new Cluster({\n        cluster: true,\n        port: 3000\n    });\n    c.on('died', function(pid) {\n        console.log('Worker ' + pid + ' died');\n    });\n    c.on('forked', function(pid) {\n        console.log('Worker ' + pid + ' forked');\n    });\n    c.on('SIGKILL', function() {\n        console.log('Got SIGKILL');\n    });\n    c.on('SIGTERM', function(event) {\n        console.log('Got SIGTERM - shutting down');\n    });\n    c.on('SIGINT', function() {\n        console.log('Got SIGINT');\n    });\n    c.listen(function(cb) {\n        cb(server);\n    });\n\n## Routing Traffic\n\nIt is fairly common for proxies or load balancers deployed in front of node clusters, and those\nproxies to use monitor URLs to detect the health of the cluster. Cluster2 includes a monitor\nat `http://<host>:<port>/ecv`. You can change this by setting the `path` property when initializing\nthe cluster.\n\nIn case you want to take the node cluster out of rotation from the proxy/load balancer, you can do\nso by setting `control` to `true` when initializing the cluster. At runtime, you can send a `POST`\nrequest to `http://<host>:<port>/ecv/disable`. Once this is done, further requests to\n`http://<host>:<port>/ecv` will get a network error. You can bring the cluster back to rotation by\nsending a `POST` request to `http://<host>:<port>/ecv/enable`.\n\nSince it will be potentially disastrous to let artibrary clients enable/disable traffic, you should\nconfigure your proxy/load balancer to prevent external traffic to `/ecv*`.\n\nTo test this, bring up an example\n\n    node examples/express/express-server.js\n\nand send a `GET` request to `http://localhost:3000/ecv` and notice the response.\n\n    HTTP/1.1 200 OK\n    X-Powered-By: Cluster2\n    content-type: text/plain\n    since: Fri May 18 2012 09:49:32 GMT-0700 (PDT)\n    cache-control: no-cache\n    Connection: keep-alive\n    Transfer-Encoding: chunked\n\n    status=AVAILABLE&ServeTraffic=true&ip=127.0.0.1&hostname=somehost&port=3000&time=Fri May 18 2012 09:49:49 GMT-0700 (PDT)\n\nTo flip the monitor into a disabled state, send a `POST` request to `http://localhost:3000/disable`.\n\n    HTTP/1.1 204 No Content\n    X-Powered-By: Cluster2\n    since: Fri May 18 2012 09:54:25 GMT-0700 (PDT)\n    cache-control: no-cache\n    Connection: close\n\nSubsequent `GET` requests to `http://localhost:3000/ecv` will return a response similar to the one\nbelow.\n\n    HTTP/1.1 400 Bad Request\n    X-Powered-By: Cluster2\n    content-type: text/plain\n    since: Fri May 18 2012 09:54:25 GMT-0700 (PDT)\n    cache-control: no-cache\n    Connection: close\n    Transfer-Encoding: chunked\n\n    status=DISABLED&ServeTraffic=false&ip=127.0.0.1&hostname=somehost&port=3000&time=Fri May 18 2012 09:55:17 GMT-0700 (PDT)\n\nTo flip the monitor back into an enabled state, send a `POST` request to `http://localhost:3000/enable`.\n\n",
  "_id": "cluster2@0.3.1",
  "description": "![Travis status](https://secure.travis-ci.org/ql-io/cluster2.png)",
  "dist": {
    "shasum": "36cbd792fdc236c40917ec33908f818e1558537b"
  },
  "_from": "cluster2"
}
